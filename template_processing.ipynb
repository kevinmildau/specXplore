{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template code structure for the pre-processing steps to be done by the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_data = \"your-spectral-dataset-filepath.mgf\"\n",
    "filename_standards = \"your-reference-standards-filepath.mgf\"\n",
    "filename_spec2vec1 = \"spec2vec-model-filepath-1\" # trained model\n",
    "filename_spec2vec1 = \"spec2vec-model-filepath-2\" # ???\n",
    "filename_spec2vec1 = \"spec2vec-model-filepath-3\" # ???\n",
    "filename_ms2deepsc = \"ms2deepscore-model-filepath\"\n",
    "path_ms2query = \"ms2query-downloaded-files-folder-path\"\n",
    "\n",
    "import os\n",
    "def download_trained_models(output_path):\n",
    "    \"\"\"Function downloads spec2vec and ms2deepscore model files needed for \n",
    "    specXplore to given filepath. \n",
    "    \n",
    "    Both positive mode and negative mode model files are downloaded and stored\n",
    "    in the given folder in respective /pos and /neg subpaths.\n",
    "\n",
    "    Input path specification should be compatible with operating system. \n",
    "    Function generalizeS across OS.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data location urls\n",
    "    spec2vec_url_pos = \"\"\n",
    "    spec2vec_url_neg = \"\"\n",
    "    ms2deepscore_url_pos = \"\"\n",
    "    ms2deepscore_url_neg = \"\"\n",
    "\n",
    "    # Output paths for postivie and negative model\n",
    "    pos_out_path = os.path.join(output_path, 'pos')\n",
    "    neg_out_path = os.path.join(output_path, 'neg')\n",
    "\n",
    "    # Model data fetching & writing.\n",
    "    # MISSING <---------------------------------------------------------------- ADD\n",
    "\n",
    "    return None # explicit void function. Implicit None return if no return.\n",
    "\n",
    "# automatic model file and library downloading in fixed format\n",
    "download_trained_models(output_path = \"models\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specxplore import processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Annotate spectrum data\n",
    "\n",
    "- An mgf file is loaded and matchms spectrum cleaning and harmonization functions are applied as a pipeline. \n",
    "- ms2query is run to generate classification tables and analog predictions. \n",
    "- The spectrum metadata is extracted as a pandas df.\n",
    "\n",
    "Returns:\n",
    "cleaned spectrum list, classification table, analog table, metadata table. All linked together through sample_idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = matchms.load_from_mgf # standard matchms loader\n",
    "\n",
    "import copy\n",
    "def clean_spectra(input_spectrums):\n",
    "    # deepcopy avoids any modification of original spectrums unless explicitly\n",
    "    # done by user outside of function.\n",
    "    spectrums = copy.deepcopy(input_spectrums) \n",
    "    spectrums = [cleaning_operation_1(spec) for spec in spectrums]\n",
    "    spectrums = [cleaning_operation_2(spec) for spec in spectrums]\n",
    "    spectrums = [cleaning_operation_3(spec) for spec in spectrums]\n",
    "    #etc...\n",
    "    return spectrums\n",
    "spectra = clean_spectra() # custom pipeline; metadata cleaning, etc.\n",
    "# --> get idx\n",
    "classification_table, analog_table = run_ms2query + fetch_results # add print for results.csv location\n",
    "metadata_table = fetch_metadata() # any custom addons should be introduced here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spectrum_id + spectrum_id + cvs file with columns # ids (anything with id or identifier column) are iloc (mgf spectrum numbers in order)\n",
    "# check id matching or check iloc use.\n",
    "# check length of metadata\n",
    "\n",
    "metadata_df = load_csv_provided()\n",
    "import numpy as np\n",
    "def join_metadata(metadata_df, id_column_name, input_spectrums):\n",
    "    # deepcopy avoids any modification of original spectrums unless explicitly\n",
    "    # done by user outside of function.\n",
    "    spectrums = copy.deepcopy(input_spectrums) \n",
    "    \n",
    "    unique_ids = set(metadata_df[id_column_name])\n",
    "    assert len(unique_ids) == metadata_df.shape[0], (\"Metadata table\"\n",
    "        \"contains non-unique identifiers!\")\n",
    "\n",
    "    metdata_list_of_dicts = metadata_df.to_dict(orient='records')\n",
    "    for spec in spectrums:\n",
    "        current_id = spec.get(\"id_column_name_in_mgf\")\n",
    "        row_iloc = np.where(metadata_df.id_column_nameate=='1992-01-03')\n",
    "        # check exception handling in np.where (not found behavior)\n",
    "        # add warning for id not found. Count number of ids not found, print summary number\n",
    "        # check for row_iloc != None before assignment\n",
    "        spec.metadata = Metadata(metdata_list_of_dicts[row_iloc]) # <--  something like this, but additive\n",
    "    \n",
    "    return spectrums\n",
    "\n",
    "# if mgf has id that exists in csv: provide join function\n",
    "# if not, use iloc and provide warning and basic checks\n",
    "join_metadata()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Annotate reference standards\n",
    "\n",
    "- An mgf file is loaded and matchms spectrum cleaning and harmonization functions are applied as a pipeline. \n",
    "- GNPS API interfacing code is used to run classyfire and npclassifier on inchi/smiles.\n",
    "- The spectrum metadata is extracted as a pandas df.\n",
    "- Any additonal spectrum metadata is added if available.\n",
    "- All spectra are indexed by their standards_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standards = matchms.load_from_mgf()\n",
    "\n",
    "\n",
    "standards = clean_spectra()\n",
    "\n",
    "# run classification will be a long running piece of code, add progress bar\n",
    "import matchms.Spectrum \n",
    "\n",
    "\n",
    "def run_classification(input_spectrums):\n",
    "    # deepcopy avoids any modification of original spectrums unless explicitly\n",
    "    # done by user outside of function.\n",
    "    spectrums = copy.deepcopy(input_spectrums) \n",
    "    for spec in input_spectrums:\n",
    "        # extract inchikeys and/or smiles\n",
    "        ...\n",
    "\n",
    "        # trycatch&wait style run GNPS api call for npclassifier and classyfire\n",
    "        tmp_npclassifier = ...\n",
    "        tmp_classyfire = ...\n",
    "\n",
    "        # add classification data to spectrum metadata if available\n",
    "        # Ensure that trycatch&wait output is always list of string, even if None.\n",
    "        \n",
    "        # add tmp indexing still! <-------------------------------------------- code not complete\n",
    "        metadata =  matchms.Spectrum.Metadata({\n",
    "            \"cf_kingdom\":None, \n",
    "            \"cf_superclass\":None, \n",
    "            \"cf_class\":None, \n",
    "            \"cf_subclass\":None, \n",
    "            \"cf_direct_parent\":None, \n",
    "            \"npc_class\":None, \n",
    "            \"npc_superclass\":None, \n",
    "            \"npc_pathway\":None, \n",
    "            'npc_isglycoside':None})\n",
    "\n",
    "classification_table = run_classification() # try_catch based\n",
    "# save the classification\n",
    "metadata_table = fetch_metadata() # any custom addons should be introduced here.\n",
    "# e.g. is_standard = TRUE column \n",
    "metadata_table[\"is_standard\"] = True # automatically repeats in df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge spectral data\n",
    "\n",
    "Sample data and reference standard data are combined together. Reference standards can be identified easily in post via their is_standard == True entry in the joint metadata table or via their inchi/smiles.\n",
    "\n",
    "A new idx is generated to uniquely identify each spectrum in the merged data. Any otherwise useful spectrum ids will be within the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tables() # possibly little overlap in columns, lots of NA information for metadata\n",
    "merge_spectra()\n",
    "get_idx() # for merged data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pairwise Similarities using matchms\n",
    "\n",
    "Requires:model files and their paths, spectrum list\n",
    "Returns:idx ordered pairwise similarities in np matrix format\n",
    "\n",
    "Note:\n",
    "- spec2vec and ms2deepscore come with their own tutorials on how to do this. \n",
    "- installations of both tools may be tricky depdending on the operating system\n",
    "- matchms has a nice interface for this already; all we can do is wrap it away and limit it\n",
    "- WARNING:all three similarity matrices are currently necessary for the dashboard; they cannot be missing.\n",
    "\n",
    "Proposed Solution:\n",
    "\n",
    "--> leave these steps in the original functions style and mainly provide output glue.\n",
    "--> a wrapper function will add additional baggage and will only be handy if we can guarantee it'll run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ms2deepscore = get_pairwise_similarities(\n",
    "    merged_spectra, \"ms2deepscore\")\n",
    "sm_modified_cosine = get_pairwise_similarities(\n",
    "    merged_spectra, \"modified_cosine\")\n",
    "sm_spec2vec = get_pairwise_similarities(\n",
    "    merged_spectra, \"spec2vec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run K-Medoid Clustering Grid\n",
    "\n",
    "Here, K-Medoid clustering is run for many levels of K to achieve a good Silhouette score.\n",
    "\n",
    "Idea: this particular code can be run and rerun easily; the grid can be modified until a suitable K is found.\n",
    "\n",
    "Return: A classification table with suitable K clustering coefficients. Small K for broad trends, large K for granularity in the t-SNE embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_grid = [5,10,15...]\n",
    "run_k_medoid_grid()\n",
    "plot(scores)\n",
    "construct_clustering_table # with desired clustering levels "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run t-SNE Grid\n",
    "\n",
    "Here, a t-SNE tuning round is done to assess what levels of perplexity would lead to good distance preservation properties of the embedding. Learning rate and number of iterations may also be investigated, but this tuning will be slower.\n",
    "\n",
    "Speed depends on data size and settings. A single run may take a couple of minutes for large datasets and certain settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed as parameter; keep seed for construct_tsne_xy as well\n",
    "perplexities = [...]\n",
    "learning_rates = [...]\n",
    "iterations = [...]\n",
    "seed = [...]\n",
    "run_tsne_grid()\n",
    "plot(scores) # + clustering at level k\n",
    "# \n",
    "construct_tsne_xy()# for selected settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct specXplore data structure\n",
    "\n",
    "Construct a specXplore data structure for use within the dashboard. Essentially a class with named data entries to use. This avoids passing around many parameters at each step of the dashboard, and provides a single place to look at the data structure used throughout specXplore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specxplore38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91721ca190d4fa69ddf052d455b156e9c3c2fc833760821d49aef838e9e6470e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
